{
  "name": "Workflow C1: Audit Entry (Job Submission)",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "audit/submit",
        "responseMode": "responseNode",
        "options": {}
      },
      "id": "b78a966b-8234-47ba-9bae-9fc5d4278006",
      "name": "Webhook: Submit Audit",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2,
      "position": [
        16800,
        1120
      ],
      "webhookId": "audit-submit-webhook"
    },
    {
      "parameters": {
        "jsCode": "// Parse multipart form data and extract files + questions mapping\nconst crypto = require('crypto');\nconst items = $input.all();\nconst firstItem = items[0];\n\n// Get questions JSON from form data (try multiple locations)\nlet questionsRaw = firstItem.json?.questions || firstItem.json?.body?.questions || firstItem.json?.query?.questions;\n\n// Debug logging\nconsole.log('First item keys:', Object.keys(firstItem.json || {}));\nconsole.log('Questions raw value:', questionsRaw);\n\nif (!questionsRaw) {\n  throw new Error('Missing \"questions\" parameter. Expected JSON array: [{\"q_id\":\"q1\",\"files\":[\"file1.pdf\"]}]. Received structure: ' + JSON.stringify(Object.keys(firstItem.json || {})));\n}\n\nlet questions;\ntry {\n  questions = typeof questionsRaw === 'string' ? JSON.parse(questionsRaw) : questionsRaw;\n} catch (e) {\n  throw new Error('Invalid questions JSON format: ' + e.message);\n}\n\nif (!questions || !Array.isArray(questions) || questions.length === 0) {\n  throw new Error('Questions must be non-empty array. Got: ' + typeof questions);\n}\n\n// Extract domain (optional, can be inferred from questions)\nconst domain = firstItem.json?.domain || firstItem.json?.body?.domain || firstItem.json?.query?.domain || 'General';\n\n// Calculate file stats and validate total size\n// We keep the original binary object to avoid data loss (corruption to 9 bytes)\nlet totalSize = 0;\nconst fileMetadata = [];\n\nif (firstItem.binary) {\n  for (const [fieldName, binaryData] of Object.entries(firstItem.binary)) {\n    if (binaryData && binaryData.data) {\n      const fileSize = Buffer.byteLength(binaryData.data, 'base64');\n      totalSize += fileSize;\n      \n      fileMetadata.push({\n        fieldName: fieldName,\n        fileName: binaryData.fileName || fieldName,\n        fileSize: fileSize,\n        mimeType: binaryData.mimeType || 'application/octet-stream'\n      });\n    }\n  }\n}\n\n// Enforce 500MB total limit for large file support\nif (totalSize > 500 * 1024 * 1024) {\n  throw new Error(`Total file size exceeds 500MB limit (current: ${Math.round(totalSize/1024/1024)}MB)`);\n}\n\n// Validate: each question references valid uploaded files\nfor (const q of questions) {\n  if (!q.q_id) {\n    throw new Error('Each question must have a \"q_id\" field');\n  }\n  if (!q.files || !Array.isArray(q.files) || q.files.length === 0) {\n    throw new Error(`Question ${q.q_id} has no files specified`);\n  }\n  \n  for (const fileName of q.files) {\n    const found = fileMetadata.find(f => f.fieldName === fileName);\n    if (!found) {\n      throw new Error(`Question ${q.q_id} references file \"${fileName}\" which was not uploaded`);\n    }\n  }\n}\n\n// Update JSON content but preserve original binary attachment\nfirstItem.json = {\n  questions,\n  domain,\n  files: fileMetadata,\n  totalSizeMB: Math.round(totalSize / 1024 / 1024 * 100) / 100,\n  totalFiles: fileMetadata.length\n};\n\nreturn [firstItem];"
      },
      "id": "b135bedb-52bf-44ab-8792-fbc75357b314",
      "name": "Parse & Validate Input",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        17024,
        1120
      ]
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "INSERT INTO audit_sessions (session_id, domain, initiated_by, status, started_at, total_questions, answered_questions, overall_compliance_score)\nVALUES (gen_random_uuid(), '{{ $json.domain.replace(/'/g, \"''\") }}', 'api_user', 'queued', NOW(), {{ $json.questions.length }}, 0, 0)\nRETURNING session_id, domain, total_questions;",
        "options": {}
      },
      "id": "82ceda42-6c52-4a2d-bdc6-8aecd13587d7",
      "name": "Create Audit Session",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.5,
      "position": [
        17248,
        1120
      ],
      "credentials": {
        "postgres": {
          "id": "3ME8TvhWnolXkgqg",
          "name": "postgres-compliance"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Calculate hashes and prepare items for writing\nconst crypto = require('crypto');\nconst fs = require('fs');\nconst path = require('path');\n\nconst sessionData = $input.first().json;\nconst sessionId = sessionData.session_id;\nconst domain = sessionData.domain;\nconst questions = $('Parse & Validate Input').first().json.questions;\nconst binaryData = $('Parse & Validate Input').first().binary;\n\n// Create session directory (fs.mkdirSync is usually safe/stable)\nconst sessionDir = `/tmp/n8n_processing/${sessionId}`;\nif (!fs.existsSync(sessionDir)) {\n  fs.mkdirSync(sessionDir, { recursive: true });\n}\n\nconsole.log(`Created session directory: ${sessionDir}`);\n\nconst items = [];\n\nfor (const [fieldName, fileData] of Object.entries(binaryData)) {\n  if (!fileData || !fileData.data) continue;\n  \n  const buffer = Buffer.from(fileData.data, 'base64');\n  const hash = crypto.createHash('sha256').update(buffer).digest('hex');\n  const ext = path.extname(fileData.fileName) || '.bin';\n  const diskFileName = `${hash}${ext}`;\n  const filePath = path.join(sessionDir, diskFileName);\n  \n  items.push({\n    json: {\n      sessionId,\n      domain,\n      fieldName,\n      fileName: fileData.fileName,\n      mimeType: fileData.mimeType,\n      fileSize: buffer.length,\n      hash,\n      filePath,\n      questions // Pass questions for aggregation later\n    },\n    binary: {\n      data: fileData // Move this specific file to 'data' key for Write node\n    }\n  });\n}\n\nreturn items;"
      },
      "name": "Prepare File Writes",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        17472,
        1120
      ],
      "id": "754c7f4b-0c57-401d-a997-ce58b38e06bf"
    },
    {
      "parameters": {
        "operation": "write",
        "fileName": "={{ $json.filePath }}",
        "options": {}
      },
      "name": "Write Binary File",
      "type": "n8n-nodes-base.readWriteFile",
      "typeVersion": 1,
      "position": [
        17680,
        1120
      ],
      "id": "74b4c682-fc63-497c-b259-85f16328ca44"
    },
    {
      "parameters": {
        "jsCode": "// Reconstruct fileMap and question structure\nconst items = $input.all();\nconst firstItem = items[0].json;\nconst sessionId = firstItem.sessionId;\nconst domain = firstItem.domain;\nconst questions = firstItem.questions;\n\nconst fileMap = {};\n\nfor (const item of items) {\n  const row = item.json;\n  fileMap[row.fieldName] = {\n    hash: row.hash,\n    fileName: row.fileName,\n    fileSize: row.fileSize,\n    mimeType: row.mimeType,\n    fieldName: row.fieldName,\n    filePath: row.filePath\n  };\n}\n\n// Map questions to hashes\nconst questionsWithHashes = questions.map(q => ({\n  q_id: q.q_id,\n  evidence_files: q.files.map(fileName => {\n    const fileInfo = fileMap[fileName];\n    return {\n      hash: fileInfo.hash,\n      originalName: fileInfo.fileName,\n      fieldName: fileName\n    };\n  })\n}));\n\nreturn [{\n  json: {\n    sessionId,\n    domain,\n    questions: questionsWithHashes,\n    fileMap,\n    totalFiles: Object.keys(fileMap).length\n  }\n}];"
      },
      "name": "Aggregate Files",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        17904,
        1120
      ],
      "id": "76acc9ae-d643-40d2-831a-7ee7a2c0eaeb"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "INSERT INTO audit_logs (session_id, q_id, step_name, status, message, percentage, created_at)\nVALUES ('{{ $json.sessionId }}', NULL, 'queued', 'in_progress', '{{ \"Job queued: \" + $json.questions.length + \" questions, \" + $json.totalFiles + \" files\" }}', 0, NOW());",
        "options": {}
      },
      "id": "c0c6047b-0802-420a-9bff-4eec22b4fafa",
      "name": "Log: Job Queued",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.5,
      "position": [
        18128,
        1120
      ],
      "credentials": {
        "postgres": {
          "id": "3ME8TvhWnolXkgqg",
          "name": "postgres-compliance"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Create Redis job payload\nconst crypto = require('crypto');\n\n// Get data from Aggregate Files node\nconst data = $('Aggregate Files').first().json;\n\n// Debug: log what we received\nconsole.log('Build Redis Job - received data keys:', Object.keys(data));\nconsole.log('Build Redis Job - questions value:', data.questions);\n\nif (!data.questions || !Array.isArray(data.questions)) {\n  throw new Error('Invalid data received: questions is ' + typeof data.questions + '. Full data keys: ' + Object.keys(data).join(', '));\n}\n\nconst jobId = crypto.randomUUID();\n\nconst job = {\n  jobId: jobId,\n  sessionId: data.sessionId,\n  domain: data.domain,\n  questions: data.questions,\n  fileMap: data.fileMap,\n  sessionDir: `/tmp/n8n_processing/${data.sessionId}`,\n  status: 'queued',\n  createdAt: new Date().toISOString()\n};\n\nreturn [{\n  json: {\n    jobData: JSON.stringify(job),\n    jobId: jobId,\n    sessionId: data.sessionId,\n    totalQuestions: data.questions.length\n  }\n}];"
      },
      "id": "84dc3d26-c399-4bbd-8a96-3dc52145361e",
      "name": "Build Redis Job Payload",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        18352,
        1120
      ]
    },
    {
      "parameters": {
        "operation": "push",
        "list": "audit_job_queue",
        "messageData": "={{ $json.jobData }}"
      },
      "id": "c466ed1c-c19c-4dc5-bab8-77802814355a",
      "name": "Enqueue Job to Redis",
      "type": "n8n-nodes-base.redis",
      "typeVersion": 1,
      "position": [
        18576,
        1120
      ],
      "credentials": {
        "redis": {
          "id": "K8jo4houPYYpv2hq",
          "name": "redis-compliance"
        }
      }
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "UPDATE audit_sessions SET job_id = '{{ $json.jobId }}' WHERE session_id = '{{ $json.sessionId }}';",
        "options": {}
      },
      "id": "f94d5f9a-ac4f-4903-9ff5-b144e5ca23da",
      "name": "Update Session Job ID",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.5,
      "position": [
        18800,
        1120
      ],
      "credentials": {
        "postgres": {
          "id": "3ME8TvhWnolXkgqg",
          "name": "postgres-compliance"
        }
      }
    },
    {
      "parameters": {
        "mode": "raw",
        "jsonOutput": "={{ {\n  \"sessionId\": $('Build Redis Job Payload').first().json.sessionId,\n  \"jobId\": $('Build Redis Job Payload').first().json.jobId,\n  \"status\": \"queued\",\n  \"totalQuestions\": $('Build Redis Job Payload').first().json.totalQuestions,\n  \"message\": \"Audit submitted successfully. Poll /webhook/audit-status-webhook/audit/status/\" + $('Build Redis Job Payload').first().json.sessionId + \" for progress.\",\n  \"estimatedCompletionMinutes\": Math.ceil($('Build Redis Job Payload').first().json.totalQuestions * 2.5)\n} }}",
        "options": {}
      },
      "id": "81a33153-285b-460a-9dc1-7fc2851a6647",
      "name": "Build Success Response",
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        19024,
        1120
      ]
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ $json }}",
        "options": {
          "responseCode": 202
        }
      },
      "id": "5f419239-42d7-44e9-9e33-c34b321b5112",
      "name": "Respond: Accepted",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1,
      "position": [
        19232,
        1120
      ]
    },
    {
      "parameters": {
        "mode": "raw",
        "jsonOutput": "={{ { \"error\": $json.error?.message || \"Internal server error\", \"status\": 500 } }}",
        "options": {}
      },
      "id": "7e70b414-93c8-470f-88e6-ad5f6ce56860",
      "name": "Error Handler",
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        17040,
        1312
      ]
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ $json }}",
        "options": {
          "responseCode": "={{ $json.status || 500 }}"
        }
      },
      "id": "95b19548-901f-47a8-8d9a-4fac43a597c4",
      "name": "Respond: Error",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1,
      "position": [
        17328,
        1408
      ]
    }
  ],
  "pinData": {},
  "connections": {
    "Webhook: Submit Audit": {
      "main": [
        [
          {
            "node": "Parse & Validate Input",
            "type": "main",
            "index": 0
          },
          {
            "node": "Error Handler",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse & Validate Input": {
      "main": [
        [
          {
            "node": "Create Audit Session",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Create Audit Session": {
      "main": [
        [
          {
            "node": "Prepare File Writes",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare File Writes": {
      "main": [
        [
          {
            "node": "Write Binary File",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Write Binary File": {
      "main": [
        [
          {
            "node": "Aggregate Files",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Aggregate Files": {
      "main": [
        [
          {
            "node": "Log: Job Queued",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Log: Job Queued": {
      "main": [
        [
          {
            "node": "Build Redis Job Payload",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Build Redis Job Payload": {
      "main": [
        [
          {
            "node": "Enqueue Job to Redis",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Enqueue Job to Redis": {
      "main": [
        [
          {
            "node": "Update Session Job ID",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Update Session Job ID": {
      "main": [
        [
          {
            "node": "Build Success Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Build Success Response": {
      "main": [
        [
          {
            "node": "Respond: Accepted",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Error Handler": {
      "main": [
        [
          {
            "node": "Respond: Error",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1",
    "binaryMode": "separate",
    "availableInMCP": false
  },
  "versionId": "c11961c1-9cd5-4733-9e21-2be896b3d4c3",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "633797609a32887ee9bd2dd2130a91be77c69c0480540cd258fc427bbd7f9ad9"
  },
  "id": "7ibMTMmHJlg58JywK1lVx",
  "tags": []
}