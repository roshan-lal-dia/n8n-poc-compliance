{
  "name": "Workflow C1: Audit Entry (Job Submission)",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "audit/submit",
        "responseMode": "responseNode",
        "options": {},
        "authentication": "headerAuth"
      },
      "id": "b78a966b-8234-47ba-9bae-9fc5d4278006",
      "name": "Webhook: Submit Audit",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2,
      "position": [
        16800,
        1120
      ],
      "webhookId": "audit-submit-webhook",
      "credentials": {
        "httpHeaderAuth": {
          "id": "webhook-api-key",
          "name": "webhook-api-key"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Parse multipart form data and extract files + questions mapping\nconst crypto = require('crypto');\nconst items = $input.all();\nconst firstItem = items[0];\n\n// Get questions JSON from form data (try multiple locations)\nlet questionsRaw = firstItem.json?.questions || firstItem.json?.body?.questions || firstItem.json?.query?.questions;\n\n// Debug logging\nconsole.log('First item keys:', Object.keys(firstItem.json || {}));\nconsole.log('Questions raw value:', questionsRaw);\n\nif (!questionsRaw) {\n  throw new Error('Missing \"questions\" parameter. Expected JSON array: [{\"q_id\":\"q1\",\"files\":[\"file1.pdf\"]}]. Received structure: ' + JSON.stringify(Object.keys(firstItem.json || {})));\n}\n\nlet questions;\ntry {\n  questions = typeof questionsRaw === 'string' ? JSON.parse(questionsRaw) : questionsRaw;\n} catch (e) {\n  throw new Error('Invalid questions JSON format: ' + e.message);\n}\n\nif (!questions || !Array.isArray(questions) || questions.length === 0) {\n  throw new Error('Questions must be non-empty array. Got: ' + typeof questions);\n}\n\n// Extract domain (optional, can be inferred from questions)\nconst domain = firstItem.json?.domain || firstItem.json?.body?.domain || firstItem.json?.query?.domain || 'General';\n\n// Calculate file stats and validate total size\n// We keep the original binary object to avoid data loss (corruption to 9 bytes)\nlet totalSize = 0;\nconst fileMetadata = [];\n\nif (firstItem.binary) {\n  for (const [fieldName, binaryData] of Object.entries(firstItem.binary)) {\n    if (binaryData && binaryData.data) {\n      const fileSize = Buffer.byteLength(binaryData.data, 'base64');\n      totalSize += fileSize;\n      \n      fileMetadata.push({\n        fieldName: fieldName,\n        fileName: binaryData.fileName || fieldName,\n        fileSize: fileSize,\n        mimeType: binaryData.mimeType || 'application/octet-stream'\n      });\n    }\n  }\n}\n\n// Enforce 500MB total limit for large file support\nif (totalSize > 500 * 1024 * 1024) {\n  throw new Error(`Total file size exceeds 500MB limit (current: ${Math.round(totalSize/1024/1024)}MB)`);\n}\n\n// Validate: each question references valid uploaded files\nfor (const q of questions) {\n  if (!q.q_id) {\n    throw new Error('Each question must have a \"q_id\" field');\n  }\n  if (!q.files || !Array.isArray(q.files) || q.files.length === 0) {\n    throw new Error(`Question ${q.q_id} has no files specified`);\n  }\n  \n  for (const fileName of q.files) {\n    const found = fileMetadata.find(f => f.fieldName === fileName);\n    if (!found) {\n      throw new Error(`Question ${q.q_id} references file \"${fileName}\" which was not uploaded`);\n    }\n  }\n}\n\n// Update JSON content but preserve original binary attachment\nfirstItem.json = {\n  questions,\n  domain,\n  files: fileMetadata,\n  totalSizeMB: Math.round(totalSize / 1024 / 1024 * 100) / 100,\n  totalFiles: fileMetadata.length\n};\n\nreturn [firstItem];"
      },
      "id": "b135bedb-52bf-44ab-8792-fbc75357b314",
      "name": "Parse & Validate Input",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        17024,
        1120
      ]
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "INSERT INTO audit_sessions (session_id, domain, initiated_by, status, started_at, total_questions, answered_questions, overall_compliance_score)\nVALUES (gen_random_uuid(), '{{ $json.domain.replace(/'/g, \"''\") }}', 'api_user', 'queued', NOW(), {{ $json.questions.length }}, 0, 0)\nRETURNING session_id, domain, total_questions;",
        "options": {}
      },
      "id": "82ceda42-6c52-4a2d-bdc6-8aecd13587d7",
      "name": "Create Audit Session",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.5,
      "position": [
        17248,
        1120
      ],
      "credentials": {
        "postgres": {
          "id": "3ME8TvhWnolXkgqg",
          "name": "postgres-compliance"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Calculate hashes and prepare items for writing\nconst crypto = require('crypto');\nconst fs = require('fs');\nconst path = require('path');\n\nconst sessionData = $input.first().json;\nconst sessionId = sessionData.session_id;\nconst domain = sessionData.domain;\nconst questions = $('Parse & Validate Input').first().json.questions;\nconst binaryData = $('Parse & Validate Input').first().binary;\n\n// Create session directory (fs.mkdirSync is usually safe/stable)\nconst sessionDir = `/tmp/n8n_processing/${sessionId}`;\nif (!fs.existsSync(sessionDir)) {\n  fs.mkdirSync(sessionDir, { recursive: true });\n}\n\nconsole.log(`Created session directory: ${sessionDir}`);\n\nconst items = [];\n\nfor (const [fieldName, fileData] of Object.entries(binaryData)) {\n  if (!fileData || !fileData.data) continue;\n  \n  const buffer = Buffer.from(fileData.data, 'base64');\n  const hash = crypto.createHash('sha256').update(buffer).digest('hex');\n  const ext = path.extname(fileData.fileName) || '.bin';\n  const diskFileName = `${hash}${ext}`;\n  const filePath = path.join(sessionDir, diskFileName);\n  \n  items.push({\n    json: {\n      sessionId,\n      domain,\n      fieldName,\n      fileName: fileData.fileName,\n      mimeType: fileData.mimeType,\n      fileSize: buffer.length,\n      hash,\n      filePath,\n      questions // Pass questions for aggregation later\n    },\n    binary: {\n      data: fileData // Move this specific file to 'data' key for Write node\n    }\n  });\n}\n\nreturn items;"
      },
      "name": "Prepare File Writes",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        17472,
        1120
      ],
      "id": "754c7f4b-0c57-401d-a997-ce58b38e06bf"
    },
    {
      "parameters": {
        "operation": "write",
        "fileName": "={{ $json.filePath }}",
        "options": {}
      },
      "name": "Write Binary File",
      "type": "n8n-nodes-base.readWriteFile",
      "typeVersion": 1,
      "position": [
        17680,
        1120
      ],
      "id": "74b4c682-fc63-497c-b259-85f16328ca44"
    },
    {
      "parameters": {
        "jsCode": "// Reconstruct fileMap and question structure\nconst items = $input.all();\nconst firstItem = items[0].json;\nconst sessionId = firstItem.sessionId;\nconst domain = firstItem.domain;\nconst questions = firstItem.questions;\n\nconst fileMap = {};\n\nfor (const item of items) {\n  const row = item.json;\n  fileMap[row.fieldName] = {\n    hash: row.hash,\n    fileName: row.fileName,\n    fileSize: row.fileSize,\n    mimeType: row.mimeType,\n    fieldName: row.fieldName,\n    filePath: row.filePath\n  };\n}\n\n// Map questions to hashes\nconst questionsWithHashes = questions.map(q => ({\n  q_id: q.q_id,\n  evidence_files: q.files.map(fileName => {\n    const fileInfo = fileMap[fileName];\n    return {\n      hash: fileInfo.hash,\n      originalName: fileInfo.fileName,\n      fieldName: fileName\n    };\n  })\n}));\n\nreturn [{\n  json: {\n    sessionId,\n    domain,\n    questions: questionsWithHashes,\n    fileMap,\n    totalFiles: Object.keys(fileMap).length\n  }\n}];"
      },
      "name": "Aggregate Files",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        17904,
        1120
      ],
      "id": "76acc9ae-d643-40d2-831a-7ee7a2c0eaeb"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "INSERT INTO audit_logs (session_id, q_id, step_name, status, message, percentage, created_at)\nVALUES ('{{ $json.sessionId }}', NULL, 'queued', 'in_progress', '{{ \"Job queued: \" + $json.questions.length + \" questions, \" + $json.totalFiles + \" files\" }}', 0, NOW());",
        "options": {}
      },
      "id": "c0c6047b-0802-420a-9bff-4eec22b4fafa",
      "name": "Log: Job Queued",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.5,
      "position": [
        18128,
        1120
      ],
      "credentials": {
        "postgres": {
          "id": "3ME8TvhWnolXkgqg",
          "name": "postgres-compliance"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Create Redis job payload\nconst crypto = require('crypto');\n\n// Get data from Aggregate Files node\nconst data = $('Aggregate Files').first().json;\n\n// Debug: log what we received\nconsole.log('Build Redis Job - received data keys:', Object.keys(data));\nconsole.log('Build Redis Job - questions value:', data.questions);\n\nif (!data.questions || !Array.isArray(data.questions)) {\n  throw new Error('Invalid data received: questions is ' + typeof data.questions + '. Full data keys: ' + Object.keys(data).join(', '));\n}\n\nconst jobId = crypto.randomUUID();\n\nconst job = {\n  jobId: jobId,\n  sessionId: data.sessionId,\n  domain: data.domain,\n  questions: data.questions,\n  fileMap: data.fileMap,\n  sessionDir: `/tmp/n8n_processing/${data.sessionId}`,\n  status: 'queued',\n  createdAt: new Date().toISOString()\n};\n\nreturn [{\n  json: {\n    jobData: JSON.stringify(job),\n    jobId: jobId,\n    sessionId: data.sessionId,\n    totalQuestions: data.questions.length\n  }\n}];"
      },
      "id": "84dc3d26-c399-4bbd-8a96-3dc52145361e",
      "name": "Build Redis Job Payload",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        18352,
        1120
      ]
    },
    {
      "parameters": {
        "operation": "push",
        "list": "audit_job_queue",
        "messageData": "={{ $json.jobData }}"
      },
      "id": "c466ed1c-c19c-4dc5-bab8-77802814355a",
      "name": "Enqueue Job to Redis",
      "type": "n8n-nodes-base.redis",
      "typeVersion": 1,
      "position": [
        18576,
        1120
      ],
      "credentials": {
        "redis": {
          "id": "K8jo4houPYYpv2hq",
          "name": "redis-compliance"
        }
      }
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "UPDATE audit_sessions SET job_id = '{{ $json.jobId }}' WHERE session_id = '{{ $json.sessionId }}';",
        "options": {}
      },
      "id": "f94d5f9a-ac4f-4903-9ff5-b144e5ca23da",
      "name": "Update Session Job ID",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.5,
      "position": [
        18800,
        1120
      ],
      "credentials": {
        "postgres": {
          "id": "3ME8TvhWnolXkgqg",
          "name": "postgres-compliance"
        }
      }
    },
    {
      "parameters": {
        "mode": "raw",
        "jsonOutput": "={{ {\n  \"sessionId\": $('Build Redis Job Payload').first().json.sessionId,\n  \"jobId\": $('Build Redis Job Payload').first().json.jobId,\n  \"status\": \"queued\",\n  \"totalQuestions\": $('Build Redis Job Payload').first().json.totalQuestions,\n  \"message\": \"Audit submitted successfully. Poll /webhook/audit-status-webhook/audit/status/\" + $('Build Redis Job Payload').first().json.sessionId + \" for progress.\",\n  \"estimatedCompletionMinutes\": Math.ceil($('Build Redis Job Payload').first().json.totalQuestions * 2.5)\n} }}",
        "options": {}
      },
      "id": "81a33153-285b-460a-9dc1-7fc2851a6647",
      "name": "Build Success Response",
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        19024,
        1120
      ]
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ $json }}",
        "options": {
          "responseCode": 202
        }
      },
      "id": "5f419239-42d7-44e9-9e33-c34b321b5112",
      "name": "Respond: Accepted",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1,
      "position": [
        19232,
        1120
      ]
    },
    {
      "parameters": {
        "mode": "raw",
        "jsonOutput": "={{ { \"error\": $json.error?.message || \"Internal server error\", \"status\": 500 } }}",
        "options": {}
      },
      "id": "7e70b414-93c8-470f-88e6-ad5f6ce56860",
      "name": "Error Handler",
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        17040,
        1312
      ]
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ $json }}",
        "options": {
          "responseCode": "={{ $json.status || 500 }}"
        }
      },
      "id": "95b19548-901f-47a8-8d9a-4fac43a597c4",
      "name": "Respond: Error",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1,
      "position": [
        17328,
        1408
      ]
    },
    {
      "parameters": {
        "jsCode": "const crypto = require('crypto');\nconst https = require('https');\n\n// Parse credentials from connection string (mirrors blobstorage.js approach)\nconst connectionString = $env.AZURE_STORAGE_CONNECTION_STRING || $env.AZURE_BLOB_CONNECTION_STRING;\nlet accountName, accountKey;\nif (connectionString) {\n  accountName = connectionString.match(/AccountName=([^;]+)/)?.[1];\n  accountKey  = connectionString.match(/AccountKey=([^;]+)/)?.[1];\n}\n// Fallback to individual env vars if connection string not set\nif (!accountName) accountName = $env.AZURE_STORAGE_ACCOUNT_NAME || 'unificdmpblob';\nif (!accountKey)  accountKey  = $env.AZURE_STORAGE_ACCOUNT_KEY;\n\nif (!accountKey) {\n  throw new Error(\n    'Azure credentials not found. Set AZURE_STORAGE_CONNECTION_STRING ' +\n    '(e.g. \"DefaultEndpointsProtocol=https;AccountName=...;AccountKey=...;EndpointSuffix=core.windows.net\") ' +\n    'in the n8n container environment.'\n  );\n}\n\nfunction httpsGet(url) {\n  return new Promise((resolve, reject) => {\n    https.get(url, (res) => {\n      const chunks = [];\n      res.on('data', chunk => chunks.push(chunk));\n      res.on('end', () => resolve({\n        buffer: Buffer.concat(chunks),\n        statusCode: res.statusCode,\n        contentType: res.headers['content-type'] || 'application/octet-stream'\n      }));\n      res.on('error', reject);\n    }).on('error', reject);\n  });\n}\n\nfunction generateSasUrl(container, blobPath, sr = 'b', permissions = 'r') {\n  const now = new Date();\n  const expiry = new Date(now.getTime() + 3600000); // 1 hour\n  const sv = '2020-12-06';\n  const fmt = (d) => d.toISOString().replace(/\\.\\d{3}Z$/, 'Z');\n  const st = fmt(now);\n  const se = fmt(expiry);\n  const canonicalizedResource = `/blob/${accountName}/${container}/${blobPath}`;\n  // sv=2020-12-06: 16 fields, 15 \\n, NO trailing newline (signedEncryptionScope added)\n  const stringToSign = [permissions, st, se, canonicalizedResource, '', '', 'https', sv, sr, '', '', '', '', '', '', ''].join('\\n');\n  const key = Buffer.from(accountKey, 'base64');\n  const sig = crypto.createHmac('sha256', key).update(Buffer.from(stringToSign, 'utf8')).digest('base64');\n  const params = new URLSearchParams({ sv, sr: 'b', sp: 'r', st, se, spr: 'https', sig });\n  return `https://${accountName}.blob.core.windows.net/${container}/${blobPath}?${params.toString()}`;\n}\n\nasync function fetchBlob(container, blobPath) {\n  const sasUrl = generateSasUrl(container, blobPath);\n  const { buffer, statusCode, contentType } = await httpsGet(sasUrl);\n  if (statusCode !== 200) {\n    throw new Error(`Azure Blob download failed HTTP ${statusCode} for ${container}/${blobPath}. Body: ${buffer.toString().substring(0, 300)}`);\n  }\n  return { buffer, contentType };\n}\n\nconst items = $input.all();\nconst result = [];\n\nfor (const item of items) {\n  // Pass through if binary already present (direct file upload path)\n  if (item.binary && Object.keys(item.binary).length > 0) {\n    result.push(item);\n    continue;\n  }\n\n  // Normalize: test webhook wraps body under item.json.body; production uses item.json directly\n  const bodyData  = item.json.body || item.json;\n  const blobPath  = bodyData.blobPath;\n  const blobFiles = bodyData.blobFiles;\n\n  if (!blobPath && !blobFiles) {\n    result.push(item);\n    continue;\n  }\n\n  if (!item.binary) item.binary = {};\n\n  if (blobPath) {\n    // Single-file mode: { \"blobPath\": \"folder/file.pdf\", \"azureContainer\": \"compliance\" }\n    const container = bodyData.azureContainer || 'compliance';\n    const { buffer, contentType } = await fetchBlob(container, blobPath);\n    const fileName = blobPath.split('/').pop();\n    item.binary.data = await this.helpers.prepareBinaryData(buffer, fileName, contentType);\n    item.json.azureBlobFetched = true;\n    item.json.originalFileName = fileName;\n  }\n\n  if (blobFiles) {\n    // Multi-file mode: { \"file0\": \"path.pdf\" } or { \"file0\": { \"blobPath\": \"...\", \"container\": \"...\" } }\n    for (const [fieldName, blobInfo] of Object.entries(blobFiles)) {\n      const bp = typeof blobInfo === 'string' ? blobInfo : blobInfo.blobPath;\n      const container = (typeof blobInfo === 'object' && blobInfo.container)\n        ? blobInfo.container\n        : (item.json.azureContainer || 'compliance');\n      const { buffer, contentType } = await fetchBlob(container, bp);\n      const fileName = bp.split('/').pop();\n      item.binary[fieldName] = await this.helpers.prepareBinaryData(buffer, fileName, contentType);\n    }\n    item.json.azureBlobFetched = true;\n  }\n\n  result.push(item);\n}\n\nreturn result;"
      },
      "id": "fetch-azure-blob-c1",
      "name": "Fetch Azure Blob",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        16912,
        1120
      ]
    },
    {
      "parameters": {
        "jsCode": "const crypto = require('crypto');\nconst https = require('https');\n\n// Parse credentials from connection string (mirrors blobstorage.js approach)\nconst connectionString = $env.AZURE_STORAGE_CONNECTION_STRING || $env.AZURE_BLOB_CONNECTION_STRING;\nlet accountName, accountKey;\nif (connectionString) {\n  accountName = connectionString.match(/AccountName=([^;]+)/)?.[1];\n  accountKey  = connectionString.match(/AccountKey=([^;]+)/)?.[1];\n}\n// Fallback to individual env vars if connection string not set\nif (!accountName) accountName = $env.AZURE_STORAGE_ACCOUNT_NAME || 'unificdmpblob';\nif (!accountKey)  accountKey  = $env.AZURE_STORAGE_ACCOUNT_KEY;\n\nif (!accountKey) {\n  throw new Error(\n    'Azure credentials not found. Set AZURE_STORAGE_CONNECTION_STRING ' +\n    '(e.g. \"DefaultEndpointsProtocol=https;AccountName=...;AccountKey=...;EndpointSuffix=core.windows.net\") ' +\n    'in the n8n container environment.'\n  );\n}\n\nfunction httpsGet(url) {\n  return new Promise((resolve, reject) => {\n    https.get(url, (res) => {\n      const chunks = [];\n      res.on('data', chunk => chunks.push(chunk));\n      res.on('end', () => resolve({\n        buffer: Buffer.concat(chunks),\n        statusCode: res.statusCode,\n        contentType: res.headers['content-type'] || 'application/octet-stream'\n      }));\n      res.on('error', reject);\n    }).on('error', reject);\n  });\n}\n\nfunction generateSasUrl(container, blobPath, sr = 'b', permissions = 'r') {\n  const now = new Date();\n  const expiry = new Date(now.getTime() + 3600000); // 1 hour\n  const sv = '2020-12-06';\n  const fmt = (d) => d.toISOString().replace(/\\.\\d{3}Z$/, 'Z');\n  const st = fmt(now);\n  const se = fmt(expiry);\n  const canonicalizedResource = `/blob/${accountName}/${container}/${blobPath}`;\n  // sv=2020-12-06: 16 fields, 15 \\n, NO trailing newline (signedEncryptionScope added)\n  const stringToSign = [permissions, st, se, canonicalizedResource, '', '', 'https', sv, sr, '', '', '', '', '', '', ''].join('\\n');\n  const key = Buffer.from(accountKey, 'base64');\n  const sig = crypto.createHmac('sha256', key).update(Buffer.from(stringToSign, 'utf8')).digest('base64');\n  const params = new URLSearchParams({ sv, sr: 'b', sp: 'r', st, se, spr: 'https', sig });\n  return `https://${accountName}.blob.core.windows.net/${container}/${blobPath}?${params.toString()}`;\n}\n\nasync function fetchBlob(container, blobPath) {\n  const sasUrl = generateSasUrl(container, blobPath);\n  const { buffer, statusCode, contentType } = await httpsGet(sasUrl);\n  if (statusCode !== 200) {\n    throw new Error(`Azure Blob download failed HTTP ${statusCode} for ${container}/${blobPath}. Body: ${buffer.toString().substring(0, 300)}`);\n  }\n  return { buffer, contentType };\n}\n\nconst items = $input.all();\nconst result = [];\n\nfor (const item of items) {\n  // Pass through if binary already present (direct file upload path)\n  if (item.binary && Object.keys(item.binary).length > 0) {\n    result.push(item);\n    continue;\n  }\n\n  // Normalize: test webhook wraps body under item.json.body; production uses item.json directly\n  const bodyData  = item.json.body || item.json;\n  const blobPath  = bodyData.blobPath;\n  const blobFiles = bodyData.blobFiles;\n\n  if (!blobPath && !blobFiles) {\n    result.push(item);\n    continue;\n  }\n\n  if (!item.binary) item.binary = {};\n\n  if (blobPath) {\n    // Single-file mode: { \"blobPath\": \"folder/file.pdf\", \"azureContainer\": \"compliance\" }\n    const container = bodyData.azureContainer || 'compliance';\n    const { buffer, contentType } = await fetchBlob(container, blobPath);\n    const fileName = blobPath.split('/').pop();\n    item.binary.data = await this.helpers.prepareBinaryData(buffer, fileName, contentType);\n    item.json.azureBlobFetched = true;\n    item.json.originalFileName = fileName;\n  }\n\n  if (blobFiles) {\n    // Multi-file mode: { \"file0\": \"path.pdf\" } or { \"file0\": { \"blobPath\": \"...\", \"container\": \"...\" } }\n    for (const [fieldName, blobInfo] of Object.entries(blobFiles)) {\n      const bp = typeof blobInfo === 'string' ? blobInfo : blobInfo.blobPath;\n      const container = (typeof blobInfo === 'object' && blobInfo.container)\n        ? blobInfo.container\n        : (item.json.azureContainer || 'compliance');\n      const { buffer, contentType } = await fetchBlob(container, bp);\n      const fileName = bp.split('/').pop();\n      item.binary[fieldName] = await this.helpers.prepareBinaryData(buffer, fileName, contentType);\n    }\n    item.json.azureBlobFetched = true;\n  }\n\n  result.push(item);\n}\n\nreturn result;"
      },
      "id": "fetch-azure-blob-c1",
      "name": "Fetch Azure Blob",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        16912,
        1120
      ]
    }
  ],
  "pinData": {},
  "connections": {
    "Webhook: Submit Audit": {
      "main": [
        [
          {
            "node": "Fetch Azure Blob",
            "type": "main",
            "index": 0
          },
          {
            "node": "Error Handler",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse & Validate Input": {
      "main": [
        [
          {
            "node": "Create Audit Session",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Create Audit Session": {
      "main": [
        [
          {
            "node": "Prepare File Writes",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare File Writes": {
      "main": [
        [
          {
            "node": "Write Binary File",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Write Binary File": {
      "main": [
        [
          {
            "node": "Aggregate Files",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Aggregate Files": {
      "main": [
        [
          {
            "node": "Log: Job Queued",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Log: Job Queued": {
      "main": [
        [
          {
            "node": "Build Redis Job Payload",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Build Redis Job Payload": {
      "main": [
        [
          {
            "node": "Enqueue Job to Redis",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Enqueue Job to Redis": {
      "main": [
        [
          {
            "node": "Update Session Job ID",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Update Session Job ID": {
      "main": [
        [
          {
            "node": "Build Success Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Build Success Response": {
      "main": [
        [
          {
            "node": "Respond: Accepted",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Error Handler": {
      "main": [
        [
          {
            "node": "Respond: Error",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Fetch Azure Blob": {
      "main": [
        [
          {
            "node": "Parse & Validate Input",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1",
    "binaryMode": "separate",
    "availableInMCP": false
  },
  "versionId": "c11961c1-9cd5-4733-9e21-2be896b3d4c3",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "633797609a32887ee9bd2dd2130a91be77c69c0480540cd258fc427bbd7f9ad9"
  },
  "id": "7ibMTMmHJlg58JywK1lVx",
  "tags": []
}