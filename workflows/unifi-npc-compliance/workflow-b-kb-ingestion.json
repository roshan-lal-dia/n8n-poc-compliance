{
  "name": "Workflow B: KB Ingestion",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "kb/ingest",
        "responseMode": "responseNode",
        "options": {},
        "authentication": "headerAuth"
      },
      "id": "d378d604-3ee2-4b8f-9e66-df7ded655530",
      "name": "Webhook: Ingest Standard",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2,
      "position": [
        3376,
        1200
      ],
      "webhookId": "kb-ingest-webhook",
      "credentials": {
        "httpHeaderAuth": {
          "id": "webhook-api-key",
          "name": "webhook-api-key"
        }
      }
    },
    {
      "parameters": {
        "rules": {
          "values": [
            {
              "conditions": {
                "options": {
                  "caseSensitive": false,
                  "leftValue": "",
                  "typeValidation": "strict",
                  "version": 3
                },
                "conditions": [
                  {
                    "id": "has-file",
                    "leftValue": "={{ Object.keys($binary).length > 0 }}",
                    "rightValue": true,
                    "operator": {
                      "type": "boolean",
                      "operation": "true"
                    }
                  }
                ],
                "combinator": "and"
              }
            }
          ]
        },
        "options": {
          "fallbackOutput": "extra"
        }
      },
      "type": "n8n-nodes-base.switch",
      "typeVersion": 3.4,
      "position": [
        3600,
        1200
      ],
      "id": "0a9c4d57-eef3-46bb-9254-0088bdec0ec5",
      "name": "Validate Input"
    },
    {
      "parameters": {
        "mode": "raw",
        "jsonOutput": "={{ { \"error\": \"Missing file or metadata\", \"status\": 400 } }}",
        "options": {}
      },
      "id": "8bb61157-4f90-4fc8-8898-74e72ae441b5",
      "name": "Error: Missing Data",
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        4608,
        1360
      ]
    },
    {
      "parameters": {
        "jsCode": "// Normalize binary data to 'data' field for consistent processing\nfor (const item of $input.all()) {\n  if (item.binary) {\n    const binaryKeys = Object.keys(item.binary);\n    if (binaryKeys.length > 0) {\n      const firstKey = binaryKeys[0];\n      if (firstKey !== 'data') {\n        item.binary.data = item.binary[firstKey];\n        delete item.binary[firstKey];\n      }\n    }\n  }\n}\nreturn $input.all();"
      },
      "id": "31ee58ae-3474-4dc2-ba8f-7ed00899515b",
      "name": "Normalize Binary Data",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        4608,
        1040
      ]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://n8n:5678/webhook/extract",
        "sendBody": true,
        "contentType": "multipart-form-data",
        "bodyParameters": {
          "parameters": [
            {
              "parameterType": "formBinaryData",
              "name": "file",
              "inputDataFieldName": "data"
            }
          ]
        },
        "options": {
          "response": {
            "response": {}
          },
          "timeout": 1800000
        },
        "authentication": "genericCredentialType",
        "genericAuthType": "httpHeaderAuth"
      },
      "id": "75ec6ee1-2b43-460f-99d8-8c40e1a5b763",
      "name": "Call Universal Extractor",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        4832,
        1040
      ],
      "credentials": {
        "httpHeaderAuth": {
          "id": "webhook-api-key",
          "name": "webhook-api-key"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "const extracted = $input.first().json;\nconst webhookData = $('Webhook: Ingest Standard').first();\nconst standardName = webhookData.json.query?.standardName || webhookData.json.body?.standardName || 'Unknown Standard';\nconst domain = webhookData.json.query?.domain || webhookData.json.body?.domain || 'General';\nconst version = webhookData.json.query?.version || webhookData.json.body?.version || '1.0';\n\nreturn [{\n  json: {\n    standardName,\n    domain,\n    version,\n    fullText: extracted.fullDocument,\n    totalPages: extracted.totalPages,\n    metadata: {\n      originalFileName: extracted.originalFileName,\n      totalWords: extracted.totalWords,\n      hasDiagrams: extracted.hasDiagrams,\n      uploadedAt: new Date().toISOString(),\n      fileHash: $('Calculate File Hash').first().json.fileHash\n    }\n  }\n}];"
      },
      "id": "3ed78410-eb6f-4532-a3d6-e5332349a8e5",
      "name": "Prepare Metadata",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        5056,
        1040
      ]
    },
    {
      "parameters": {
        "jsCode": "const text = $input.first().json.fullText;\nconst chunkSize = 1000;\nconst chunkOverlap = 200;\nconst metadata = $input.first().json;\n\nconst chunks = [];\nconst words = text.split(/\\s+/);\n\nfor (let i = 0; i < words.length; i += (chunkSize - chunkOverlap)) {\n  const chunk = words.slice(i, i + chunkSize).join(' ');\n  if (chunk.trim().length > 50) {\n    chunks.push({\n      json: {\n        text: chunk,\n        chunkIndex: chunks.length,\n        standardName: metadata.standardName,\n        domain: metadata.domain,\n        version: metadata.version,\n        metadata: metadata.metadata\n      }\n    });\n  }\n}\n\nreturn chunks;"
      },
      "id": "1818b43e-c68d-46a8-bf28-43f12d850f7d",
      "name": "Chunk Text",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        5280,
        1040
      ]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://ollama:11434/api/embeddings",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ { \"model\": \"nomic-embed-text\", \"prompt\": $json.text } }}",
        "options": {}
      },
      "id": "41c3cc18-37c9-4ade-aa21-f5fcd7d5d131",
      "name": "Generate Embedding",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        5488,
        1040
      ]
    },
    {
      "parameters": {
        "jsCode": "const crypto = require('crypto');\nconst embeddings = $input.all();\nconst chunks = $('Chunk Text').all();\n\nconst results = [];\n\n// Iterate over all items to ensure we process every chunk, not just the first one\nfor (let i = 0; i < embeddings.length; i++) {\n  const item = embeddings[i];\n  // Match embedding with its corresponding chunk source\n  // Assumes 1-to-1 mapping, which is standard for HTTP request nodes processing items\n  const chunkData = chunks[i] ? chunks[i].json : {};\n  \n  // Handle cases where Ollama might return error or different structure\n  const vector = item.json.embedding;\n  if (!vector) continue;\n\n  // ðŸ›¡ï¸ Proactive Fix: Create a unique key that includes the filename\n  const uniqueKey = `${chunkData.standardName}_${chunkData.metadata.originalFileName || 'doc'}_${chunkData.chunkIndex}`;\n\n  // ðŸ†” Generate a valid UUID v4-like string from the hash (Deterministic)\n  const hash = crypto.createHash('md5').update(uniqueKey).digest('hex');\n  const uuid = [\n    hash.substring(0, 8),\n    hash.substring(8, 12),\n    hash.substring(12, 16),\n    hash.substring(16, 20),\n    hash.substring(20, 32)\n  ].join('-');\n\n  results.push({\n    json: {\n      id: uuid,\n      vector: vector,\n      payload: {\n        text: chunkData.text,\n        standardName: chunkData.standardName,\n        domain: chunkData.domain,\n        version: chunkData.version,\n        chunkIndex: chunkData.chunkIndex,\n        metadata: chunkData.metadata\n      }\n    }\n  });\n}\n\nreturn results;"
      },
      "id": "57f0f548-b2b8-4191-a64d-72b2d73d48d9",
      "name": "Format Qdrant Point",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        5712,
        1040
      ]
    },
    {
      "parameters": {
        "method": "PUT",
        "url": "=http://qdrant:6333/collections/compliance_standards/points",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ { \"points\": $input.all().map(item => item.json) } }}",
        "options": {}
      },
      "id": "34e2a57d-4f23-48bd-af34-99cb89da1f9e",
      "name": "Upsert to Qdrant",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        5936,
        1040
      ]
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "\n            INSERT INTO kb_standards (standard_name, domain, file_hash, filename, total_chunks, uploaded_at) \n            VALUES ($1, $2, $3, $4, $5, NOW()) \n            ON CONFLICT (file_hash) DO UPDATE SET uploaded_at = NOW() \n            RETURNING id\n            ",
        "options": {
          "queryReplacement": "={{ $('Prepare Metadata').first().json.standardName }},{{ $('Prepare Metadata').first().json.domain }},{{ $('Prepare Metadata').first().json.metadata.fileHash }},{{ $('Prepare Metadata').first().json.metadata.originalFileName }},{{ $('Format Qdrant Point').all().length }}"
        }
      },
      "id": "7e3136a2-2523-4359-a79a-bf5e2c2a31bd",
      "name": "Insert to Postgres",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.5,
      "position": [
        6160,
        1040
      ],
      "credentials": {
        "postgres": {
          "id": "3ME8TvhWnolXkgqg",
          "name": "Compliance DB"
        }
      }
    },
    {
      "parameters": {
        "mode": "raw",
        "jsonOutput": "={{ {\n  \"status\": \"success\",\n  \"message\": \"Standard ingested successfully\",\n  \"standardName\": $('Prepare Metadata').first().json.standardName,\n  \"domain\": $('Prepare Metadata').first().json.domain,\n  \"chunksCreated\": $('Format Qdrant Point').all().length,\n  \"dbRecordId\": $json[0]?.id\n} }}",
        "options": {}
      },
      "id": "61be4649-9d4a-4d30-bbbf-b2a9efc39459",
      "name": "Format Response",
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        6368,
        1040
      ]
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ $json }}",
        "options": {}
      },
      "id": "7b60002f-0874-4d78-99a3-7a7e0b99a0ce",
      "name": "Respond to Webhook",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1,
      "position": [
        6592,
        1040
      ]
    },
    {
      "parameters": {},
      "id": "f7ad98ad-1362-4933-ab8a-0713b72f8f15",
      "name": "Note: Collection Init",
      "type": "n8n-nodes-base.noOp",
      "typeVersion": 1,
      "position": [
        3376,
        1440
      ],
      "notes": "Run this curl command once to initialize the Qdrant collection:\n\ncurl -X PUT http://qdrant:6333/collections/compliance_standards \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"vectors\": {\n      \"size\": 768,\n      \"distance\": \"Cosine\"\n    }\n  }'"
    },
    {
      "parameters": {
        "jsCode": "const crypto = require('crypto');\nconst items = $input.all();\nreturn items.map(item => {\n  const binaryKey = Object.keys(item.binary)[0];\n  const binaryData = item.binary[binaryKey];\n  // Calculate SHA-256 hash of the binary data\n  const hash = crypto.createHash('sha256').update(Buffer.from(binaryData.data, 'base64')).digest('hex');\n  \n  return {\n    json: {\n      ...item.json,\n      fileHash: hash\n    },\n    binary: item.binary\n  };\n});"
      },
      "id": "c21cf090-2f53-4aad-bad3-b9739215ff35",
      "name": "Calculate File Hash",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        3808,
        1200
      ]
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "SELECT id, standard_name FROM kb_standards WHERE file_hash = $1\nUNION ALL\nSELECT NULL as id, NULL as standard_name WHERE NOT EXISTS (SELECT 1 FROM kb_standards WHERE file_hash = $1)\nLIMIT 1",
        "options": {
          "queryReplacement": "={{ $json.fileHash }}"
        }
      },
      "id": "0a6e3f84-8b87-4b63-b3e4-e14cd1239ca5",
      "name": "Check Hash in DB",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.5,
      "position": [
        4000,
        1200
      ],
      "credentials": {
        "postgres": {
          "id": "3ME8TvhWnolXkgqg",
          "name": "Compliance DB"
        }
      }
    },
    {
      "parameters": {
        "rules": {
          "values": [
            {
              "conditions": {
                "options": {
                  "caseSensitive": true,
                  "leftValue": "",
                  "typeValidation": "loose",
                  "version": 3
                },
                "conditions": [
                  {
                    "id": "is-new",
                    "leftValue": "={{ $json.id == null || $json.id === '' }}",
                    "rightValue": true,
                    "operator": {
                      "type": "boolean",
                      "operation": "true"
                    }
                  }
                ],
                "combinator": "and"
              }
            }
          ]
        },
        "options": {
          "fallbackOutput": "extra"
        }
      },
      "type": "n8n-nodes-base.switch",
      "typeVersion": 3.4,
      "position": [
        4208,
        1200
      ],
      "id": "0ab27cad-fe07-4ca5-8a8a-018ed0d34bce",
      "name": "Is New File?"
    },
    {
      "parameters": {
        "mode": "raw",
        "jsonOutput": "={{ { \"message\": \"File already ingested\", \"standardName\": $json.standard_name, \"status\": \"skipped\", \"dbId\": $json.id } }}",
        "options": {}
      },
      "id": "e03db23d-9d98-4678-b8b7-99a0ebb7f8b7",
      "name": "Set: Already Exists",
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        4400,
        1376
      ]
    },
    {
      "parameters": {
        "jsCode": "const originalItems = $('Calculate File Hash').all();\n// We assume 1-to-1 processing\nreturn originalItems.map(item => ({\n  json: item.json,\n  binary: item.binary\n}));"
      },
      "id": "c1f577b6-fefe-46b0-bee9-e21eec665d02",
      "name": "Restore Data",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        4400,
        1024
      ]
    },
    {
      "parameters": {
        "jsCode": "const crypto = require('crypto');\nconst https = require('https');\n\n// Parse credentials from connection string (mirrors blobstorage.js approach)\nconst connectionString = $env.AZURE_STORAGE_CONNECTION_STRING || $env.AZURE_BLOB_CONNECTION_STRING;\nlet accountName, accountKey;\nif (connectionString) {\n  accountName = connectionString.match(/AccountName=([^;]+)/)?.[1];\n  accountKey  = connectionString.match(/AccountKey=([^;]+)/)?.[1];\n}\n// Fallback to individual env vars if connection string not set\nif (!accountName) accountName = $env.AZURE_STORAGE_ACCOUNT_NAME || 'unificdmpblob';\nif (!accountKey)  accountKey  = $env.AZURE_STORAGE_ACCOUNT_KEY;\n\nif (!accountKey) {\n  throw new Error(\n    'Azure credentials not found. Set AZURE_STORAGE_CONNECTION_STRING ' +\n    '(e.g. \"DefaultEndpointsProtocol=https;AccountName=...;AccountKey=...;EndpointSuffix=core.windows.net\") ' +\n    'in the n8n container environment.'\n  );\n}\n\nfunction httpsGet(url) {\n  return new Promise((resolve, reject) => {\n    https.get(url, (res) => {\n      const chunks = [];\n      res.on('data', chunk => chunks.push(chunk));\n      res.on('end', () => resolve({\n        buffer: Buffer.concat(chunks),\n        statusCode: res.statusCode,\n        contentType: res.headers['content-type'] || 'application/octet-stream'\n      }));\n      res.on('error', reject);\n    }).on('error', reject);\n  });\n}\n\nfunction generateSasUrl(container, blobPath, sr = 'b', permissions = 'r') {\n  const now = new Date();\n  const expiry = new Date(now.getTime() + 3600000); // 1 hour\n  const sv = '2020-12-06';\n  const fmt = (d) => d.toISOString().replace(/\\.\\d{3}Z$/, 'Z');\n  const st = fmt(now);\n  const se = fmt(expiry);\n  const canonicalizedResource = `/blob/${accountName}/${container}/${blobPath}`;\n  // sv=2020-12-06: 16 fields, 15 \\n, NO trailing newline (signedEncryptionScope added)\n  const stringToSign = [permissions, st, se, canonicalizedResource, '', '', 'https', sv, sr, '', '', '', '', '', '', ''].join('\\n');\n  const key = Buffer.from(accountKey, 'base64');\n  const sig = crypto.createHmac('sha256', key).update(Buffer.from(stringToSign, 'utf8')).digest('base64');\n  const params = new URLSearchParams({ sv, sr: 'b', sp: 'r', st, se, spr: 'https', sig });\n  return `https://${accountName}.blob.core.windows.net/${container}/${blobPath}?${params.toString()}`;\n}\n\nasync function fetchBlob(container, blobPath) {\n  const sasUrl = generateSasUrl(container, blobPath);\n  const { buffer, statusCode, contentType } = await httpsGet(sasUrl);\n  if (statusCode !== 200) {\n    throw new Error(`Azure Blob download failed HTTP ${statusCode} for ${container}/${blobPath}. Body: ${buffer.toString().substring(0, 300)}`);\n  }\n  return { buffer, contentType };\n}\n\nconst items = $input.all();\nconst result = [];\n\nfor (const item of items) {\n  // Pass through if binary already present (direct file upload path)\n  if (item.binary && Object.keys(item.binary).length > 0) {\n    result.push(item);\n    continue;\n  }\n\n  // Normalize: test webhook wraps body under item.json.body; production uses item.json directly\n  const bodyData  = item.json.body || item.json;\n  const blobPath  = bodyData.blobPath;\n  const blobFiles = bodyData.blobFiles;\n\n  if (!blobPath && !blobFiles) {\n    result.push(item);\n    continue;\n  }\n\n  if (!item.binary) item.binary = {};\n\n  if (blobPath) {\n    // Single-file mode: { \"blobPath\": \"folder/file.pdf\", \"azureContainer\": \"compliance\" }\n    const container = bodyData.azureContainer || 'compliance';\n    const { buffer, contentType } = await fetchBlob(container, blobPath);\n    const fileName = blobPath.split('/').pop();\n    item.binary.data = await this.helpers.prepareBinaryData(buffer, fileName, contentType);\n    item.json.azureBlobFetched = true;\n    item.json.originalFileName = fileName;\n  }\n\n  if (blobFiles) {\n    // Multi-file mode: { \"file0\": \"path.pdf\" } or { \"file0\": { \"blobPath\": \"...\", \"container\": \"...\" } }\n    for (const [fieldName, blobInfo] of Object.entries(blobFiles)) {\n      const bp = typeof blobInfo === 'string' ? blobInfo : blobInfo.blobPath;\n      const container = (typeof blobInfo === 'object' && blobInfo.container)\n        ? blobInfo.container\n        : (item.json.azureContainer || 'compliance');\n      const { buffer, contentType } = await fetchBlob(container, bp);\n      const fileName = bp.split('/').pop();\n      item.binary[fieldName] = await this.helpers.prepareBinaryData(buffer, fileName, contentType);\n    }\n    item.json.azureBlobFetched = true;\n  }\n\n  result.push(item);\n}\n\nreturn result;"
      },
      "id": "fetch-azure-blob-b",
      "name": "Fetch Azure Blob",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        3488,
        1200
      ]
    },
    {
      "parameters": {
        "jsCode": "const crypto = require('crypto');\nconst https = require('https');\n\n// Parse credentials from connection string (mirrors blobstorage.js approach)\nconst connectionString = $env.AZURE_STORAGE_CONNECTION_STRING || $env.AZURE_BLOB_CONNECTION_STRING;\nlet accountName, accountKey;\nif (connectionString) {\n  accountName = connectionString.match(/AccountName=([^;]+)/)?.[1];\n  accountKey  = connectionString.match(/AccountKey=([^;]+)/)?.[1];\n}\n// Fallback to individual env vars if connection string not set\nif (!accountName) accountName = $env.AZURE_STORAGE_ACCOUNT_NAME || 'unificdmpblob';\nif (!accountKey)  accountKey  = $env.AZURE_STORAGE_ACCOUNT_KEY;\n\nif (!accountKey) {\n  throw new Error(\n    'Azure credentials not found. Set AZURE_STORAGE_CONNECTION_STRING ' +\n    '(e.g. \"DefaultEndpointsProtocol=https;AccountName=...;AccountKey=...;EndpointSuffix=core.windows.net\") ' +\n    'in the n8n container environment.'\n  );\n}\n\nfunction httpsGet(url) {\n  return new Promise((resolve, reject) => {\n    https.get(url, (res) => {\n      const chunks = [];\n      res.on('data', chunk => chunks.push(chunk));\n      res.on('end', () => resolve({\n        buffer: Buffer.concat(chunks),\n        statusCode: res.statusCode,\n        contentType: res.headers['content-type'] || 'application/octet-stream'\n      }));\n      res.on('error', reject);\n    }).on('error', reject);\n  });\n}\n\nfunction generateSasUrl(container, blobPath, sr = 'b', permissions = 'r') {\n  const now = new Date();\n  const expiry = new Date(now.getTime() + 3600000); // 1 hour\n  const sv = '2020-12-06';\n  const fmt = (d) => d.toISOString().replace(/\\.\\d{3}Z$/, 'Z');\n  const st = fmt(now);\n  const se = fmt(expiry);\n  const canonicalizedResource = `/blob/${accountName}/${container}/${blobPath}`;\n  // sv=2020-12-06: 16 fields, 15 \\n, NO trailing newline (signedEncryptionScope added)\n  const stringToSign = [permissions, st, se, canonicalizedResource, '', '', 'https', sv, sr, '', '', '', '', '', '', ''].join('\\n');\n  const key = Buffer.from(accountKey, 'base64');\n  const sig = crypto.createHmac('sha256', key).update(Buffer.from(stringToSign, 'utf8')).digest('base64');\n  const params = new URLSearchParams({ sv, sr: 'b', sp: 'r', st, se, spr: 'https', sig });\n  return `https://${accountName}.blob.core.windows.net/${container}/${blobPath}?${params.toString()}`;\n}\n\nasync function fetchBlob(container, blobPath) {\n  const sasUrl = generateSasUrl(container, blobPath);\n  const { buffer, statusCode, contentType } = await httpsGet(sasUrl);\n  if (statusCode !== 200) {\n    throw new Error(`Azure Blob download failed HTTP ${statusCode} for ${container}/${blobPath}. Body: ${buffer.toString().substring(0, 300)}`);\n  }\n  return { buffer, contentType };\n}\n\nconst items = $input.all();\nconst result = [];\n\nfor (const item of items) {\n  // Pass through if binary already present (direct file upload path)\n  if (item.binary && Object.keys(item.binary).length > 0) {\n    result.push(item);\n    continue;\n  }\n\n  // Normalize: test webhook wraps body under item.json.body; production uses item.json directly\n  const bodyData  = item.json.body || item.json;\n  const blobPath  = bodyData.blobPath;\n  const blobFiles = bodyData.blobFiles;\n\n  if (!blobPath && !blobFiles) {\n    result.push(item);\n    continue;\n  }\n\n  if (!item.binary) item.binary = {};\n\n  if (blobPath) {\n    // Single-file mode: { \"blobPath\": \"folder/file.pdf\", \"azureContainer\": \"compliance\" }\n    const container = bodyData.azureContainer || 'compliance';\n    const { buffer, contentType } = await fetchBlob(container, blobPath);\n    const fileName = blobPath.split('/').pop();\n    item.binary.data = await this.helpers.prepareBinaryData(buffer, fileName, contentType);\n    item.json.azureBlobFetched = true;\n    item.json.originalFileName = fileName;\n  }\n\n  if (blobFiles) {\n    // Multi-file mode: { \"file0\": \"path.pdf\" } or { \"file0\": { \"blobPath\": \"...\", \"container\": \"...\" } }\n    for (const [fieldName, blobInfo] of Object.entries(blobFiles)) {\n      const bp = typeof blobInfo === 'string' ? blobInfo : blobInfo.blobPath;\n      const container = (typeof blobInfo === 'object' && blobInfo.container)\n        ? blobInfo.container\n        : (item.json.azureContainer || 'compliance');\n      const { buffer, contentType } = await fetchBlob(container, bp);\n      const fileName = bp.split('/').pop();\n      item.binary[fieldName] = await this.helpers.prepareBinaryData(buffer, fileName, contentType);\n    }\n    item.json.azureBlobFetched = true;\n  }\n\n  result.push(item);\n}\n\nreturn result;"
      },
      "id": "fetch-azure-blob-b",
      "name": "Fetch Azure Blob",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        3488,
        1200
      ]
    }
  ],
  "pinData": {},
  "connections": {
    "Webhook: Ingest Standard": {
      "main": [
        [
          {
            "node": "Fetch Azure Blob",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Validate Input": {
      "main": [
        [
          {
            "node": "Calculate File Hash",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Error: Missing Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Error: Missing Data": {
      "main": [
        [
          {
            "node": "Respond to Webhook",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Normalize Binary Data": {
      "main": [
        [
          {
            "node": "Call Universal Extractor",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call Universal Extractor": {
      "main": [
        [
          {
            "node": "Prepare Metadata",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Metadata": {
      "main": [
        [
          {
            "node": "Chunk Text",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Chunk Text": {
      "main": [
        [
          {
            "node": "Generate Embedding",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Generate Embedding": {
      "main": [
        [
          {
            "node": "Format Qdrant Point",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Format Qdrant Point": {
      "main": [
        [
          {
            "node": "Upsert to Qdrant",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Upsert to Qdrant": {
      "main": [
        [
          {
            "node": "Insert to Postgres",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Insert to Postgres": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Format Response": {
      "main": [
        [
          {
            "node": "Respond to Webhook",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Calculate File Hash": {
      "main": [
        [
          {
            "node": "Check Hash in DB",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Check Hash in DB": {
      "main": [
        [
          {
            "node": "Is New File?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Is New File?": {
      "main": [
        [
          {
            "node": "Restore Data",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Set: Already Exists",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Set: Already Exists": {
      "main": [
        [
          {
            "node": "Respond to Webhook",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Restore Data": {
      "main": [
        [
          {
            "node": "Normalize Binary Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Fetch Azure Blob": {
      "main": [
        [
          {
            "node": "Validate Input",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1",
    "binaryMode": "separate",
    "availableInMCP": false
  },
  "versionId": "b54bcd31-4cfb-40df-959d-de3f24d2d949",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "633797609a32887ee9bd2dd2130a91be77c69c0480540cd258fc427bbd7f9ad9"
  },
  "id": "DQA2xRb89erBF8K-pDFAJ",
  "tags": []
}